---
title: "Classification Linear Kernel SVM. Iris Data Two Separable Classes"
author: "Jane Kathambi"
date: "8 June 2018"
output: 
  html_document:
    keep_md: yes
    number_sections: yes
    theme: united
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---
# Introduction
A classification problem involves predicting a non-numerical valueâ€”that is, a categorical variable, also known as discrete variable.

Most of the classification algorithms computes the probability of belonging to a given class. Observations are then assigned to the class that have the highest probability score.

Generally, you need to decide a probability cutoff above which you consider the an observation as belonging to a given class.

## Recap
** separable data:** 
* LDA: Small training set. Also Linear kernel svms.
* QDA: Large training set. Also Linear kernel svms.
* RDA: Large training set and too many features. Also Linear kernel svms.

#The Iris data set
The iris data set contains the length and width of sepals and petals for three iris species. We want to predict the species based on the sepal and petal parameters.

This data has three classes of which two are inseparable. We will transform this data set to only contain two classes which are separable. For separable data, linear kernel svms and lda model the data well.

# Classification: Linear Kernel SVM classifier.
Linear kernel svm has one tuning parameter C. C solves the problem of outliers i.e overlapping observations by allowing for soft margins.

# Load the required libraries.
* tidyverse. for easy data manipulation and visualization
* caret. for easy machine learning workflow. createDataPartition for partitioning the data into test set and train set.

```{r}
library(tidyverse)
library(caret)
```

# Exploring the Iris Data and transforming the iris data.

```{r}
# Load the data
data("iris")

# check for Nas
anyNA(iris)

# Inspect the data
sample_n(iris, 3)

# dim
dim(iris)

#internl structure
glimpse(iris)
```

Covariance matrix
```{r}
# Covariance scatterplot Matrix. View how each variable varies with the rest as well as how the classes are distributed
pairs(iris, col=iris$Species)

```

From the covariance matrix above it is evident that there are three classes two of which are inseparable but one is separable from the other two classes. 

Let us now plot the data and color it with species in order to identify the separable species.

```{r}
# plot the data, color with species
ggplot(iris, aes(x=Petal.Length, y=Petal.Width, col=Species))+geom_point()

```

From the above plot it si evident that the versicolor and virginica species are inseparable.

So setosa and versicolor are separable as well as setosa and virginica.

Let us now transoform the iris data so as to only retain separable classes that is setosa and versicolor.

# Transformed iris separable classes setosa and versicolor

```{r}
# filter iris to retain two classes i.e setosa and versicolor
separable_iris<-iris%>%
  filter(Species %in% c('versicolor','setosa'))

#separable_iris<-iris[iris$Species %in% c('versicolor','setosa'),]

# Confirm that there are two species i.e setosa and versicolor
unique(separable_iris$Species)

# View the species of the new data set
separable_iris$Species

# view the levels of the new data set
levels(separable_iris$Species)
```

We now have two species  setosa and versicolor. However the levels have not changed. So the function factor(data$categorical_variable)  drops the levels that do not occur.

Let us now drop the levels that we dont require so that thy dont affect our analysis.
```{r}
# drops the levels that do not occur
separable_iris$Species<-factor(separable_iris$Species)

# levels
levels(separable_iris$Species)
```


# Normalize the data. Categorical variables are automatically ignored.
```{r}
# Estimate preprocessing parameters
preproc.param <- separable_iris %>% 
  preProcess(method = c("center", "scale"))

# Transform the data using the estimated parameters
standardized_iris <- preproc.param %>% predict(separable_iris)


```

# Split the data into training and test set
```{r}
set.seed(123)
training.samples <- standardized_iris$Species %>% 
  createDataPartition(p = 0.8, list = FALSE)

train.data  <- standardized_iris[training.samples, ]
test.data <- standardized_iris[-training.samples, ]
```
# Fitting linear kernel svm model

To build a linear SVM classifier, we will use linear kernel function. The caret package can be used to easily compute the linear SVM linear model.

The package automatically chooses the optimal value C for the model tuning parameters, where optimal is defined as a value that maximizes the model accuracy.

```{r}
# Fit the model on the training set
set.seed(123)

model <- train(
  Species ~., data = train.data, method = "svmLinear",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
  )
# Print the best tuning parameter sigma and C that
# maximizes model accuracy
model$bestTune
```

# model accuracy

```{r}
# Make predictions on the test data
predicted.classes <- model %>% predict(test.data)

# Compute model accuracy rate
mean(predicted.classes == test.data$Species)
```

Our model has an accuracy of 1. This implies that the model has done the best that there is to do. 100% correct classifications. 

The best models for this data are both LDA and Linear kernel svm classifiers.

# Model Accuracies Ranking
The following is a ranking of how the models perfomed on classifying this iris data starting from the best model. 
1. Linear Discriminant Analysis: 1 accuracy
1. Linear Kernel SVM classifier: 1 accuracy