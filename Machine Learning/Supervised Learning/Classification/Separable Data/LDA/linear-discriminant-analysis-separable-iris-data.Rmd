---
title: "Classification Linear Discriminant Analysis. Iris Data Two Separable Classes"
author: "Jane Kathambi"
date: "8 June 2018"
output: 
  html_document:
    keep_md: yes
    number_sections: yes
    theme: united
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---
# Introduction
A classification problem involves predicting a non-numerical valueâ€”that is, a categorical variable, also known as discrete variable.

Most of the classification algorithms computes the probability of belonging to a given class. Observations are then assigned to the class that have the highest probability score.

Generally, you need to decide a probability cutoff above which you consider the an observation as belonging to a given class.

## Recap
** separable data:** 
* LDA: Small training set. Also Linear kernel svms.
* QDA: Large training set. Also Linear kernel svms.
* RDA: Large training set and too many features. Also Linear kernel svms.

**inseparable data:**
* FDA: Multivariate inseparable data sets. Also radial kernel svms.
* MDA: Classes have sub-classes which are distributed. Also radial kernel svms.

#The Iris data set
The iris data set contains the length and width of sepals and petals for three iris species. We want to predict the species based on the sepal and petal parameters.

This data has three classes of which two are inseparable. We will transform this data set to only contain two classes which are separable. For separable data, lda and linear kernel svms model the data well.

# Classification:  LDA (Linear discriminant analysis)
?????

# Load the required libraries.
* tidyverse. for easy data manipulation and visualization
* caret. for easy machine learning workflow. createDataPartition for partitioning the data into test set and train set.
* MASS library: has the lda function.

```{r}
library(tidyverse)
library(caret)
library(MASS)
```

# Exploring the Iris Data and transforming the iris data.

```{r}
# Load the data
data("iris")

# check for Nas
anyNA(iris)

# Inspect the data
sample_n(iris, 3)

# dim
dim(iris)

#internl structure
glimpse(iris)
```

Covariance matrix
```{r}
# Covariance scatterplot Matrix. View how each variable varies with the rest as well as how the classes are distributed
pairs(iris, col=iris$Species)

```

From the covariance matrix above it is evident that there are three classes two of which are inseparable but one is separable from the other two classes. 

Let us now plot the data and color it with species in order to identify the separable species.

```{r}
# plot the data, color with species
ggplot(iris, aes(x=Petal.Length, y=Petal.Width, col=Species))+geom_point()

```

From the above plot it si evident that the versicolor and virginica species are inseparable.

So setosa and versicolor are separable as well as setosa and virginica.

Let us now transoform the iris data so as to only retain separable classes that is setosa and versicolor.

# Transformed iris separable classes setosa and versicolor

```{r}
# filter iris to retain two classes i.e setosa and versicolor
separable_iris<-iris%>%
  filter(Species %in% c('versicolor','setosa'))

#separable_iris<-iris[iris$Species %in% c('versicolor','setosa'),]

# Confirm that there are two species i.e setosa and versicolor
unique(separable_iris$Species)

# View the species of the new data set
separable_iris$Species

# view the levels of the new data set
levels(separable_iris$Species)
```

We now have two species  setosa and versicolor. However the levels have not changed. So the function factor(data$categorical_variable)  drops the levels that do not occur.

Let us now drop the levels that we dont require so that thy dont affect our analysis.
```{r}
# drops the levels that do not occur
separable_iris$Species<-factor(separable_iris$Species)

# levels
levels(separable_iris$Species)
```

# Normalize the data. Categorical variables are automatically ignored.
```{r}
# Estimate preprocessing parameters
preproc.param <- separable_iris %>% 
  preProcess(method = c("center", "scale"))

# Transform the data using the estimated parameters
standardized_iris <- preproc.param %>% predict(separable_iris)


```

# Split the data into training and test set
```{r}
set.seed(123)
training.samples <- standardized_iris$Species %>% 
  createDataPartition(p = 0.8, list = FALSE)

train.data  <- standardized_iris[training.samples, ]
test.data <- standardized_iris[-training.samples, ]
```

# Fit the LDA model
```{r}
# Fit the LDA model
lda_model <- lda(Species~., data = train.data)
lda_model
```

# Plot the model on classification of train.data (predictions on train data)
```{r}
# plot the model
plot(lda_model, train.data)
```

# Plot the model on classification of test.data (predictions on new data)
```{r}
# plot the model
plot(lda_model, test.data)
```

# LDA Model accuracy
For lda model we access model accuracy as follows:
mean(preds$class == test.data$Species)

```{r}
# Make predictions
preds <- lda_model %>% predict(test.data)

# Model accuracy
mean(preds$class == test.data$Species)


```

Our model has an accuracy of 1. This implies that the model has done the best that there is to do. 100% correct classifications. 

The best models for this data are both LDA and Linear kernel svm classifiers.

# Model Accuracies Ranking
The following is a ranking of how the models perfomed on classifying this iris data starting from the best model. 
1. Linear Discriminant Analysis: 1 accuracy
1. Linear Kernel SVM classifier: 1 accuracy

