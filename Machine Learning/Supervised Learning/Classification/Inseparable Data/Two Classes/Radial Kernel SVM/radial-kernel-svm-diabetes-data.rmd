---
title: "Radial Kernel SVM Diabetes Data"
author: "Jane Kathambi"
date: "8 June 2018"
output: 
  html_document:
    keep_md: yes
    theme: united
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

# Introduction
A classification problem involves predicting a non-numerical valueâ€”that is, a categorical variable, also known as discrete variable.

Most of the classification algorithms computes the probability of belonging to a given class. Observations are then assigned to the class that have the highest probability score.

Generally, you need to decide a probability cutoff above which you consider the an observation as belonging to a given class.

## Recap:
Use radial kernel svms for inseparable data. Can be used for two or more than two classes. FDA (FDA is a flexible extension of LDA that uses non-linear combinations of predictors such as splines. FDA is useful to model multivariate non-normality or non-linear relationships among variables within each group, allowing for a more accurate classification.) works well too with any number of classes. Logistic Regression with ridge penalty works too but with only two classes.

Radial kernel SVMs have two tuning parameters which are C and Sigma. C solves the problem of outliers i.e overlapping observations by allowing for soft margins, while Sigma reduces variance by regularizing the features since with radial kernels the feature space is enlarged to improve seperability of the classes.

Generally, the purpose of regularization is to balance accuracy and simplicity.
This means a model with the smallest number of predictors that also gives a good accuracy. 

# The PimaIndiansDiabetes2 Data
PimaIndiansDiabetes2 is contained in the mlbench package. We will use it for predicting the probability of being diabetes positive or negative based on multiple clinical variables. This is a binary classification problem.

This data set has two inseparable classes, as we will see shortly, and so a radial kernel svm will model this data well.

The data contains 768 individuals (female) and 9 clinical variables for predicting the probability of individuals in being diabete-positive or negative:

* pregnant: number of times pregnant
* glucose: plasma glucose concentration
* pressure: diastolic blood pressure (mm Hg)
* triceps: triceps skin fold thickness (mm)
* insulin: 2-Hour serum insulin (mu U/ml)
* mass: body mass index (weight in kg/(height in m)^2)
* pedigree: diabetes pedigree function
* age: age (years)
* diabetes: class variable

# Load the required packages
* tidyverse. For data mangling.
* caret. for easy machine learning workflow.

```{r}
library(tidyverse)
library(caret)
```

# Exploring the PimaIndiansDiabetes2 Data

```{r}
# Load the data
data("PimaIndiansDiabetes2", package = "mlbench")

# remove Nas
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)

# Inspect the data
sample_n(PimaIndiansDiabetes2, 3)

# Covariance scatterplot Matrix. View how each variable varies with the rest as well as how the classes are distributed.
pairs(PimaIndiansDiabetes2, col=PimaIndiansDiabetes2$diabetes)

# plot two variables to see how many classes there are and if they are separable.
PimaIndiansDiabetes3=PimaIndiansDiabetes2[,c('glucose','pressure')]
pairs(PimaIndiansDiabetes3, col=PimaIndiansDiabetes2$diabetes)

# dim
dim(PimaIndiansDiabetes2)
```

From the pairs plot above we see that there are two classes which are inseparable. So radial kernel svm will model this data well. FDA and ridge penalised logistic regression would too work well.

## Normalize the data. Categorical variables are automatically ignored.
```{r}
# Estimate preprocessing parameters
preproc.param <- PimaIndiansDiabetes2 %>% 
  preProcess(method = c("center", "scale"))

# Transform the data using the estimated parameters
PimaIndiansDiabetes2_normalized <- preproc.param %>% predict(PimaIndiansDiabetes2)

```


## Split the data into training and test set
We'll randomly split the data into training set (80% for building a predictive model) and test set (20% for evaluating the model). Make sure to set seed for reproductibility.
```{r}
set.seed(123)
training.samples <- PimaIndiansDiabetes2_normalized$diabetes %>% 
  createDataPartition(p = 0.8, list = FALSE)

train.data  <- PimaIndiansDiabetes2_normalized[training.samples, ]
test.data <- PimaIndiansDiabetes2_normalized[-training.samples, ]

```


# Fitting radial kernel svm model

To build a non-linear SVM classifier, we will use radial kernel function. The caret package can be used to easily compute the radial SVM non-linear model.

The package automatically chooses the optimal values for the model tuning parameters, where optimal is defined as values that maximize the model accuracy.

```{r}
# Fit the model on the training set
set.seed(123)

model <- train(
  diabetes ~., data = train.data, method = "svmRadial",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
  )
# Print the best tuning parameter sigma and C that
# maximizes model accuracy
model$bestTune
```

# model accuracy

```{r}
# Make predictions on the test data
predicted.classes <- model %>% predict(test.data)

# Compute model accuracy rate
mean(predicted.classes == test.data$diabetes)
```

Our model has an accuracy of 0.7948. This implies that the model is doing fairly well. Although the best model on this data is fda model.

# Model Accuracies Ranking
The following is a ranking of how the models perfomed on classifying this diabetes data strating from the best model. The best model for this data is FDA.

1. Flexible Discriminant Analysis: 0.807 accuracy
2. Radial Kernel SVM classifier: 0.794 accuracy
3. Ridge Penalised Logistic Regression: 0.782 accuracy
4. Mixture Discriminant Analysis: 0.769 accuracy

