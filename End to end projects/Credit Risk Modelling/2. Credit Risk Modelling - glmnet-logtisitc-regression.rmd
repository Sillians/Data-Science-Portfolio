---
title: "Credit Risk Modelling"
author: "Jane Kathambi"
date: "25 July 2018"
output: 
  html_document:
    keep_md: yes
    theme: united
    toc: yes
    toc_depth: 6
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

# Introduction
Modeling credit risk for both personal and company loans is of major importance for banks. The probability that a debtor will default is a key component in getting to a measure for credit risk. 

We will use real-life credit data to explore how to model credit risk by using lasso penalized logistic regression. This is an improved version of logistic regression since it performs important feature selection in order to improve model performance.

Glmnet has inbuilt feature selection and parameter tuning. The inbuilt feature selection uses the L1 regularization method to shrink irrelevant features to zero. 

We will try some parameter tuning using caret grid search and see if the model will improve.


# Credit Data 
We will use the German credit dataset. The data contains data on 20 variables and the classification whether an applicant is considered a Good or a Bad credit risk for 1000 loan applicants.

# Load the required libraries

* caret: for machine learning
* gmodels package: Contains the CrossTable() function
* tidyverse: For data mangling and visualization.
* caTools: is to score the models using a Receiver Operating Characteristic (ROC) curve.
```{r}
library(gmodels)
library(tidyverse)
library(caret)
library(caTools)
```

# Load the credit data
```{r}
loan_data<-read.table('data/german.data.txt')
```

# Data exploration and Preprocessing

## The strucutre and summary of the data
We will view the structure of the data set to get familiar with it, we will use glimpse a function of the dplyr package.
```{r}
# View the structure of loan_data
#glimpse(loan_data)

```

## Change varaible names
The variables names don't represent the true meaning of the data, so let us change the variable names as follows:
```{r}
# assign the data new column names
colnames(loan_data) <- c( 'account_balance', 
                    'months_loan_duration', 
                    'credit_history', 
                    'purpose', 
                    'credit_amount', 
                    'savings_balance', 
                    'employment_status', 
                    'installment_rate', 
                    'personal_status', 
                    'other_debtors_guarantors', 
                    'present_residence_years',  
                    'property', 
                    'age', 
                    'other_installment', 
                    'housing', 
                    'number_credits_this_bank', 
                    'job', 
                    'number_dependents', 
                    'phone', 
                    'foreign_worker', 
                     'default')

# View the structure of loan_data to comfirm new col names
#glimpse(loan_data)
```

## Fix factor variables
We have to fix the factor variables as most models only accept numeric data. In a nutshell, dummifying factors breaks all the unique values into separate columns.
```{r}
#dummyfy factor variables
loan_data_dummy <- dummyVars("~.",data=loan_data, fullRank=F)
loan_data <- as.data.frame(predict(loan_data_dummy, loan_data))

# show names of new variables
#print(names(loan_data))

#view the structure of the new data frame
#glimpse(loan_data)
```

## Response variable coding

As this is a binary classification, we need to force models into using the classification mode. We do this by changing the outcome variable to a factor.
```{r}
#How the default variabel is coded. 
# results show that defaults are coded as 2, and non-defaults as 1
table(loan_data$default)

# Change the levels 
loan_data$default <- ifelse(loan_data$default==2,'def','non_def')

# coarse default to factor
loan_data$default <- as.factor(loan_data$default)

#confirm levels have changed
levels(loan_data$default)

```

Default is coded as 2 while non-default 1.

We would like to add levels (def, non_def) that depict the meaning of the variable as we have done above.

## Exploring the credit data (default rates, outliers, missing values).

### Default rates

After being given loan_data, you are particularly interested about the defaulted loans in the data set. You want to get an idea of the number, and percentage of defaults. Defaults are rare, so you always want to check what the proportion of defaults is in a loan dataset. 

To learn more about variable structures and spot unexpected tendencies in the data, you should examine the relationship between default variable and certain factor variables. 

*What is the proportion of defaults in the data?*

In order for the model to be able to make accurate forecasts it needs to see enough examples of what constitutes a default. For this reason it is important that there is a sufficiently large number of defaults in the data. Typically in practice, data with less than 5% of defaults pose strong modelling challenges.

1. Have a look at the CrossTable() of loan status, using just one argument: loan_data$default.


```{r}

# Call CrossTable() on loan_status
CrossTable(loan_data$default, prop.r=TRUE, prop.c=FALSE, prop.t=FALSE, prop.chisq=FALSE)
```

Our data has 30% defaults. So this data is good for modelling.

2. Call CrossTable() with x argument installment_rate and y argument default. We only want row-wise proportions, so set prop.r to TRUE, but prop.c , prop.t and prop.chisq to FALSE (default values here are TRUE, and this would lead to inclusion of column proportions, table proportions and chi-square contributions for each cell. We do not need these here.) We expect high installment rates to be associated with high default rates. Let us eplore the data to see if this hypothesis holds true.

```{r}
# Call CrossTable() on grade and loan_status
CrossTable(loan_data$installment_rate , loan_data$default, prop.r=TRUE, prop.c=FALSE, prop.t=FALSE, prop.chisq=FALSE)

```

As you can see the proportion of defaults increase when the installment rate increase.

### Missing values
```{r}
anyNA(loan_data)
```

There are no missing values

### Oultiers

Lets explore some continous variable to detect outliers:
1. credit_amount
2. installment_rate
3. present_residence_years
4. age
5. number_credits_this_bank
6. number_dependents
7. months_loan_duration

We will use boxplots
```{r}
attach(loan_data)

par(mfrow=c(1,1))
boxplot(age)
hist(age)

```

There are no outliers in this data set.

## split data into training set and test set
```{r}
seed=107

set.seed(seed)

inTrain <- createDataPartition(
  
  ## the outcome data are needed
  y = loan_data$default,
  
  ## The percentage of data in the training set
  p = .75,

  list = FALSE
)

train.data <- loan_data[ inTrain,]
train.predictors<-train.data[,1:61]
train.response<-train.data$default

test.data <- loan_data[-inTrain,]
test.predictors<-test.data[, 1:61]
test.response<-test.data$default

```


# Exploring the classes to inform model to choose

We will seek to know if the classes are separable or not

```{r}
ggplot(loan_data, aes(x = age, y = credit_amount, col=default)) +
  geom_point() 

```


From the above scatter plot it is evident that there are two classes and they are inseparable. So logistic regression can model this data very well.


We will evaluate the efficiency of the logistic regression model based on AUC (Area Under ROC).

# Configure parallel processing
Parallel processing in caret can be accomplished with the parallel and doParallel packages. The following code loads the required libraries (note, these libraries also depend on the iterators and foreach libraries).

```{r}
#Include the parallel library. 
library(parallel)

#Include the doParallel library. 
library(doParallel)

# Use the detectCores() function to find the number of cores in system
no_cores <- detectCores()-1 # convention to leave 1 core for OS
 
# Setup cluster
clust <- makeCluster(no_cores) 


```


# Glmnet Logistic regression 

## Train control
Caret offers many tuning functions to help you get as much as possible out of your models; the *trainControl* function allows you to control the resampling of your data. 

This will split the training data set internally and do it's own train/test runs to figure out the best settings for your model. 

In this case, we're going to use repeated cross-validation repeating 3 times,in order to tume/find optimal tuning parameters for using in the final model.For logistic regression using glmnet the tuning parameters are:

1. *alpha*, 
2. *lambda*. 

You can also set these values yourself if you don't trust the function.

```{r}
set.seed(seed)

train_ctrl <- trainControl(
  method = "repeatedcv", 
  repeats = 3,
  returnResamp = "all",
  summaryFunction = twoClassSummary,#(ROC, Sensitivity, Specificity). Used for twoClass-problems
  
  classProbs = TRUE, #Since the ROC curve is based on the predicted class probabilities (which are not computed automatically)
  
  allowParallel = TRUE # allow paralellel processing
)
```

## fit the base logistic regression
We will use all the variables.
```{r}
set.seed(seed)

x<-train.predictors # predictors
y<-train.response # response

baseGlmnetLogisticRegFit <- train(x, y, 
                method='glmnet', family="binomial",
                trControl=train_ctrl,
                metric = "ROC",
                tuneLength = 5,
                preProc = c("center", "scale")
                )
```

## output the model
```{r}
baseGlmnetLogisticRegFit
```


## Best parameters
```{r}
baseGlmnetLogisticRegFit$bestTune

```

## Chosen variables
```{r}
predictors(baseGlmnetLogisticRegFit)
```


## Plot base glmnet logistic regression model
```{r}
ggplot(baseGlmnetLogisticRegFit,  metric = "ROC")

```

## Obtain predicted probabilities and classes of test data
Accuracy requires classes.
ROC reguires probabilities.

```{r}
#classes
baseGlmnetLogisticRegClasses <- predict(object=baseGlmnetLogisticRegFit, test.predictors)

#performance accuracy
confusionMatrix(data=baseGlmnetLogisticRegClasses, test.response)

# probabilities
baseGlmnetLogisticRegProbs <- predict(object=baseGlmnetLogisticRegFit, test.predictors, type='prob')

#performance AUC
colAUC(baseGlmnetLogisticRegProbs, test.response)
```
Our model has an accuracy of 76.8%, kappa of 36.54%. Based on these two parameters this model has an average performance.

The AUC of Predictions with test data is telling us that our model has a 77.59% AUC score (remember that an AUC ranges between 0.5 and 1, where 0.5 is random and 1 is perfect).


# view parameters available for tuning
```{r}
modelLookup("glmnet")
```

They are alpha and lambda

# Caret grid search parameter tuning

```{r}
##############################################################################
set.seed(seed)

train_ctrl <- trainControl(
  method = "repeatedcv", 
  repeats = 3,
  returnResamp = "all",
  summaryFunction = twoClassSummary,#(ROC, Sensitivity, Specificity). Used for twoClass-problems
  
  classProbs = TRUE, #Since the ROC curve is based on the predicted class probabilities (which are not computed automatically)
  
  search= "grid",
  
  allowParallel = TRUE # allow paralellel processing
)

##############################################################################
x<-train.predictors # predictors
y<-train.response # response

tuneGrid = expand.grid(.alpha = seq(0.05, 1, by=0.01), .lambda = seq(0.01, 0.5, by=0.01))
#0.1	 0.06183724	

gridSearchGlmnetLogisticRegFit <- train(x, y, 
                method='glmnet', family="binomial",
                trControl=train_ctrl,
                metric = "ROC",
                tuneGrid=tuneGrid,
                #tuneLength = 5,
                preProc = c("center", "scale")
                )

```

## output the model
```{r}
gridSearchGlmnetLogisticRegFit
```


## Best parameters
```{r}
gridSearchGlmnetLogisticRegFit$bestTune

```

## Chosen variables
```{r}
predictors(gridSearchGlmnetLogisticRegFit)
```


## Plot gridSearchGlmnetLogisticRegFit
```{r}
ggplot(gridSearchGlmnetLogisticRegFit,  metric = "ROC")

```

## Obtain predicted probabilities and classes of test data
Accuracy requires classes.
ROC reguires probabilities.

```{r}
#classes
gridSearchGlmnetLogisticRegClasses <- predict(object=gridSearchGlmnetLogisticRegFit, test.predictors)

#performance accuracy
confusionMatrix(data=gridSearchGlmnetLogisticRegClasses, test.response)

# probabilities
gridSearchGlmnetLogisticRegProbs <- predict(object=gridSearchGlmnetLogisticRegFit, test.predictors, type='prob')

#performance AUC
colAUC(gridSearchGlmnetLogisticRegProbs, test.response)
```
Our model has an accuracy of 74.4%, kappa of 23.26%. Based on these two parameters this model has an average performance.

The AUC of Predictions with test data is telling us that our model has a 77.26% AUC score (remember that an AUC ranges between 0.5 and 1, where 0.5 is random and 1 is perfect).


# Models comparisons

## Training results

### Resampling training results(ROC, Sens, Spec)
How do these models compare in terms of their resampling results? 

The resamples function can be used to collect, summarize and contrast the resampling results. Since the random number seeds were initialized to the same value prior to calling train, the same folds were used for each model. To assemble them:

```{r}
#collect and summarize the resampling results
resamps <- resamples(list( baseGlmnetLogisticRegFit=baseGlmnetLogisticRegFit, gridSearchGlmnetLogisticRegFit=gridSearchGlmnetLogisticRegFit ))

# Print the summary of the resampling results
summary(resamps)
```

### Box plot to visualize the results
```{r}
bwplot(resamps )

```

### Dot plot to visualize the results
```{r}
dotplot(resamps )

```


## Predictions on test data results
We can use accuracy but for this exercise let us use AUC.

### Predicted AUC

```{r}

# baseGlmnetLogisticRegFit AUC
aucBaseGlmnetLogisticReg<-colAUC(baseGlmnetLogisticRegProbs, test.response)

# gridSearchLogisticRegFit AUC
aucGridSearchGlmnetLogisticReg<-colAUC(gridSearchGlmnetLogisticRegProbs, test.response)


#put the data together in a dataframe while selecting only one value of each
data.frame(aucBaseGlmnetLogisticReg=aucBaseGlmnetLogisticReg[1],aucGridSearchGlmnetLogisticReg=aucGridSearchGlmnetLogisticReg[1] )
```

Ranking the models based on predicted AUC starting with the best performing model:

1. Default Glmnet Logistic Regression         --> Predicted AUC=0.7759238
2. Glmet Logistic Regression with grid serach --> Predicted AUC=0.7726476	

This experiment shows us that the best model out of the two is the default glmnet logistic regression i.e lasso penalised logistic regression.

In the next study we will tune the random forest model and reduce model complexity by reducig irrelevant features and correlated features. 

